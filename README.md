# Fastify + Ollama API

API construÃ­da com **Fastify** e **Zod** com o objetivo de se conectar a um **modelo de IA executado via Ollama**, permitindo testes, estudos e integraÃ§Ã£o com serviÃ§os de inteligÃªncia artificial.

---

## ğŸš€ Tecnologias utilizadas

* **Fastify** â€“ framework web rÃ¡pido e eficiente
* **Zod** â€“ validaÃ§Ã£o e tipagem de schemas
* **fastify-type-provider-zod** â€“ integraÃ§Ã£o nativa entre Fastify e Zod
* **Ollama** â€“ execuÃ§Ã£o e proxy de modelos de IA
* **Axios** â€“ comunicaÃ§Ã£o HTTP
* **Swagger UI** â€“ documentaÃ§Ã£o interativa
* **Scalar** â€“ documentaÃ§Ã£o moderna de API
* **TypeScript**

---

## ğŸ¯ Objetivo do projeto

Este projeto serve como uma base para:

* Criar APIs Fastify bem tipadas
* Validar requests e responses com Zod
* Integrar a aplicaÃ§Ã£o com um **modelo de IA via Ollama**
* Testar chamadas para LLMs de forma local ou via cloud

---

## ğŸ“¦ PrÃ©-requisitos

Antes de rodar a aplicaÃ§Ã£o, Ã© necessÃ¡rio:

* **Node.js** (versÃ£o recomendada: 18+)
* **npm** ou **yarn**
* **Ollama** instalado na mÃ¡quina

### Instalar o Ollama

Acesse:
ğŸ‘‰ [https://ollama.com](https://ollama.com)

E siga as instruÃ§Ãµes para o seu sistema operacional.

---

## ğŸ¤– Configurando o Ollama

### 1ï¸âƒ£ Baixar um modelo

VocÃª pode utilizar qualquer modelo disponÃ­vel no Ollama. Exemplo com modelo cloud:

```bash
ollama pull gpt-oss:120b-cloud
```

Ou utilize outro modelo de sua preferÃªncia.

---

### 2ï¸âƒ£ Rodar o modelo

```bash
ollama run gpt-oss:120b-cloud
```

> âš ï¸ ObservaÃ§Ã£o: modelos `*-cloud` exigem login no Ollama e sÃ£o executados nos servidores da Ollama, mas continuam sendo acessados localmente pela API.

---

## âš™ï¸ VariÃ¡veis de ambiente

Crie um arquivo `.env` na raiz do projeto e configure as seguintes variÃ¡veis:

```env
OLLAMA_API_URL=https://api.ollama.com
OLLAMA_API_KEY=api_key_aqui
OLLAMA_MODEL=gpt-oss:120b-cloud

JWT_SECRET=super-secret-key
NODE_ENV=development
```

> ğŸ’¡ Mesmo utilizando Ollama local, essas variÃ¡veis permitem flexibilidade para troca de ambiente e modelo.

---

## â–¶ï¸ Rodando a aplicaÃ§Ã£o

Instale as dependÃªncias:

```bash
npm install
```

Inicie a aplicaÃ§Ã£o em modo desenvolvimento:

```bash
npm run dev
```

A API estarÃ¡ disponÃ­vel em:

```
http://localhost:3333
```

---

## ğŸ“š DocumentaÃ§Ã£o da API

VocÃª pode testar e explorar a API de duas formas:

### ğŸ”¹ Swagger UI

```
http://localhost:3333/swagger
```

Interface clÃ¡ssica para testes manuais dos endpoints.

---

### ğŸ”¹ Scalar Docs

```
http://localhost:3333/docs
```

Interface moderna e mais amigÃ¡vel para navegaÃ§Ã£o e leitura da API.

---

## ğŸ§ª Testando a integraÃ§Ã£o com o Ollama

A aplicaÃ§Ã£o possui uma rota de teste para validar a comunicaÃ§Ã£o com o modelo de IA.

Exemplo:

```http
GET /test/ollama
```

Essa rota envia um prompt simples ao modelo configurado e retorna a resposta, confirmando que:

* O Ollama estÃ¡ rodando
* O modelo estÃ¡ acessÃ­vel
* A API estÃ¡ corretamente integrada

---

## ğŸ§© Estrutura geral

* `src/server.ts` â€“ bootstrap da aplicaÃ§Ã£o
* `src/routes` â€“ rotas da API
* `src/plugins` â€“ plugins e handlers globais
* `src/test/ollama` â€“ rota de teste de integraÃ§Ã£o com IA
* `src/utils` â€“ schemas e utilidades

---

## ğŸ‘¨â€ğŸ’» Autor

**Gabriel Suptitz**

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a ISC.

---

## âœ… ObservaÃ§Ãµes finais

* Para modelos locais, prefira opÃ§Ãµes menores (ex: `llama3`, `mistral`) para melhor performance
* Modelos cloud podem exigir **timeouts maiores**
* O projeto foi pensado para facilitar a troca de modelos apenas via variÃ¡veis de ambiente

Bom uso ğŸš€
